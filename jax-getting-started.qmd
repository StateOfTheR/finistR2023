---
title: "Premiers pas avec jax"
subtitle: "Différentiation automatique avec Jax"
date: "du 21 au 25 août 2023"
execute: 
    freeze: auto
output: 
  html_document:
   toc: true
   toc_float: true
---

Le but de cette vignette est d'implémenter une régression logistique et/ou une régression multivariée avec JAX.

## Préliminaires

### Installation 

#### Conda

Jax est disponible dans le channel conda-forge et peut donc s'installer dans un environnement conda

```{bash}
#| eval: true
conda create -n jax
conda activate jax
## Install the CPU version
conda install jax -c conda-forge
```

Pour des instruction détaillées pour l'installation en mode GPU ou TPU, se référer à la [documentation officielle](https://github.com/google/jax#installation). 

Il suffit alors d'activer l'environnement jax pour produire le html à partir du qmd

```{bash}
#| eval: false
conda activate jax
quarto render my_document.qmd --to html
```

#### Pip

Si vous préférez une installation via pip (pour une version cpu), 

```{bash}
#| eval: false
pip3 install jax jaxlib
```

Pour une installation GPU (avec cuda 12 par exemple, on vous laisse gérer la compatibilité de vos driver Nvidia and Cie),

```{bash}
#| eval: false
pip install --upgrade "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

L'utilisation de `venv` est recommandable (mais non obligatoire).

## Premiers pas 


### Philosophie

En quelques mots, JAX est une bibliothèque Python développé par Google et itinitalement utilisé dans TensorFLow. Elle permet de faire de l'algèbre linéaire à la `numpy`, avec 2 propriétés clés le rendant extrêment performant:

1. un *autograd* permettant la différention automatique de calcul Python/Numpy
2. un *compileur* pour GPU et autres ([XLA](https://www.tensorflow.org/xla)), dédié à l'algèbre linéaire qui permet d'optimiser les temps d'exécution grace à une approche *JIT* (Just-in Time, c'est-à-dire une optimisation du code à l'exécution et non pas avant l'appel comme avec un compileur classique).

L'objectif de la bibliothèque est de proposer une expérience utilisateur aussi proche que possible de calculs à la `Numpy`, notamment à l'aide de décorateurs Python. Néanmoins, pour accéder pleinement aux capacités de JAX, un certains nombres de contraintes d'écriture des programmes s'appliquent, que nous allons essayer de présenter pas à pas. 

### Import de la bibliothèque

L'import complet/standard est le suivant: 

```{python}
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
```

On peut détailler les fonctionnalité des modules comme suit:

- le module `jax.numpy`, aka `jnp`, porte les opérations matricielles usuelles de manière quasi transparente 
- le module `random` définit les outils de génération de nombres aléatoires, propres à JAX et très différents de Numpy 
- le module `grad` gère l'autodifférentiation
- le module `jit` gère la "just-in time" compilation (accélartion du code)
- le module `vmap` permet de vectoriser automatiquement certaines opérations

### Jax.numpy: interface Algèbre linéaire haut-niveau

On commence par simuler des données aléatoires via les outils de jax. Attention la gestion de la clé aléatoire est **explicite**. Après avoir créé une clé et avant chaque appel à une fonction aléatoire, il faut faire 

```{python}
#| eval: false
key,subkey = random.split(key, 2)
```

et utiliser `subkey` dans l'appel à la fonction aléatoire comme écrit ci-dessous. 

```{python}
n = 10000
p = 100
key = random.PRNGKey(0)
key,subkey = random.split(key, 2)
ones = jnp.ones((n, 1))
x = random.normal(subkey, (n, p-1))
x = jnp.concatenate([ones, x], axis = 1)
key,subkey = random.split(key, 2)
beta_true = random.normal(subkey, (p,1))
```

Avant de les multiplier. On utilise ici la fonction `block_until_ready()` uniquement pour mesurer le temps effectif de calcul. En effet, JAX fait de l'évaluation asynchrone (comme {{future}} en R) pour rendre la main à l'utilisateur après l'envoi de la commande. 

```{python}
%timeit odds = jnp.dot(x, beta_true).block_until_ready()  # runs on the CPU
```

On échantillonne ensuite des variables suivant une loi de Bernoulli. 

```{python}
odds = jnp.dot(x, beta_true)
key,subkey = random.split(key, 2)
y = random.bernoulli(subkey, odds)
```

et une perte logistique

$$\ell(y, x, \theta) = -\log p(y; \sigma(x^{\top}\theta)) = -y (x^\top \theta) - \log(1 + e^{x^\top \theta})$$

```{python}
def logistic_loss(y, x, theta):
  odds = jnp.dot(x, theta)
  return -jnp.vdot(y, odds) + jnp.sum(jnp.log(1.0 + jnp.exp(odds)))
```

Qu'on peut tester sur un example simple

```{python}
logistic_loss(True, 1.0, 0)
```

### Just-in-time compilation

La version *normale* de notre fonction logistique est déjà rapide. 

```{python}
%timeit logistic_loss(y, x, beta_true).block_until_ready()
```

mais on peut l'accélerer en compilant la fonction via `@jit` de façon complètement transparente pour l'utilisateur. 

```{python}
logistic_loss_jit = jit(logistic_loss)
%timeit logistic_loss_jit(y, x, beta_true).block_until_ready()
```

La différence est assez importante dans cet example mais moins pour des échantillons plus gros parce que JAX va automatiquement mettre en cache 

### grad: auto-différentiation

JAX permet de calculer le gradient d'une fonction via `grad()`. La syntaxe est différente de torch et plus proche de ce qu'on ferait dans une fonction mathématique. 

```{python}
def loss(theta):
  return logistic_loss(y, x, theta)
```

```{python}
## random start for theta
key,subkey = random.split(key, 2)
theta = random.normal(key, (p, 1))
grad_loss = grad(loss)
print(grad_loss(theta))
```

`grad()` peut-être combiné à `jit()` dans tous les sens. 

```{python}
grad_loss = grad(loss)
%timeit grad_loss(theta)
```

```{python}
grad_loss = jit(grad(loss))
%timeit grad_loss(theta)
```

```{python}
grad_loss = jit(grad(jit(loss)))
%timeit grad_loss(theta)
```

### Vectorisation
